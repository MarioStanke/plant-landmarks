{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following five imports have to be installed\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas.util\n",
    "import os\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"images/images_landmarks/\"\n",
    "LANDMARK_PATH = \"files/preprocessed_landmarks.npy\"\n",
    "\n",
    "TEST_FOLDER = 'images/test/'\n",
    "TRAIN_FOLDER ='images/train/'\n",
    "VAL_FOLDER = 'images/val/'\n",
    "\n",
    "NUM_TRANSLATIONS = 6\n",
    "\n",
    "TEST_PKL = \"files/test_landmarks_df.pkl\"\n",
    "TRAIN_PKL = \"files/train_landmarks_df.pkl\"\n",
    "VAL_PKL = \"files/val_landmarks_df.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the IMAGE_FOLDER exists, if not: raise an error.\n",
    "if not os.path.exists(IMAGE_FOLDER):\n",
    "    raise FileNotFoundError('The preprocessing of the images (landmarks_image_preprocessing.ipynb) has to be conducted first or the name of the IMAGE_FOLDER has to be adjusted.')\n",
    "\n",
    "#check if the LANDMARK_PATH exists, if not: raise an error.\n",
    "if not os.path.exists(LANDMARK_PATH):\n",
    "    raise FileNotFoundError('The preprocessing of the images (landmarks_image_preprocessing.ipynb) has to be conducted first or the name of the LANDMARK_PATH has to be adjusted.')\n",
    "    \n",
    "#check if the directories TEST_FOLDER, TRAIN_FOLDER, VAL_FOLDER exist, if not: create them.\n",
    "#Otherwise delete its contents\n",
    "if not os.path.exists(TEST_FOLDER):\n",
    "    os.mkdir(TEST_FOLDER)\n",
    "else:\n",
    "    shutil.rmtree(TEST_FOLDER)\n",
    "    os.mkdir(TEST_FOLDER)\n",
    "    \n",
    "if not os.path.exists(TRAIN_FOLDER):\n",
    "    os.mkdir(TRAIN_FOLDER)\n",
    "else:\n",
    "    shutil.rmtree(TRAIN_FOLDER)\n",
    "    os.mkdir(TRAIN_FOLDER)\n",
    "    \n",
    "if not os.path.exists(VAL_FOLDER):\n",
    "    os.mkdir(VAL_FOLDER)\n",
    "else:\n",
    "    shutil.rmtree(VAL_FOLDER)\n",
    "    os.mkdir(VAL_FOLDER) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load coordinate information of landmarks and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.load(LANDMARK_PATH, allow_pickle = True)[()].copy()\n",
    "\n",
    "# create a dataframe\n",
    "df_landmarks = pd.DataFrame.from_dict(landmarks, orient = 'index')\n",
    "df_landmarks['filename'] = df_landmarks.index\n",
    "\n",
    "# all unique filenames\n",
    "unique_files = list({key[:-len(\"_t0.jpg\")] for key in landmarks.keys()})\n",
    "random.shuffle(unique_files) #randomly shuffle these filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write picture information in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = os.listdir(IMAGE_FOLDER)\n",
    "\n",
    "f= open(\"images.txt\",\"w+\")\n",
    "for i in range(len(filelist)):\n",
    "    f.write(f\"{filelist[i]};{filelist[i][3:6]};{IMAGE_FOLDER}{filelist[i]}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create panda dataframe from textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"images.txt\", sep = ';', names = ['filename','classname', 'image_path'], encoding = 'unicode_escape')\n",
    "df = df.set_index('filename')\n",
    "\n",
    "print(df.head())\n",
    "print(\"shape of data frame: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df, df_landmarks]\n",
    "df_all = pd.concat(frames, axis = 1, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create training, validation and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split data frame into training, validation and test data frames\n",
    "num_imgs  = len(unique_files) # total number of examples\n",
    "num_test  = 100# size of test set, used only once at end\n",
    "num_val   = 100 # size of validation set, used to monitor training progress\n",
    "num_train = num_imgs - num_test - num_val # size of training set, the (large) rest\n",
    "\n",
    "assert num_train > 0, \"Error: examples consumed by test and validation sets alone\"\n",
    "\n",
    "all_filenames = [f\"{filename}_t{i}.jpg\" for filename in unique_files for i in range(NUM_TRANSLATIONS)]\n",
    "\n",
    "test_indices = all_filenames[:NUM_TRANSLATIONS*num_test]\n",
    "val_indicies = all_filenames[NUM_TRANSLATIONS*num_test:NUM_TRANSLATIONS*(num_test+num_val)]\n",
    "train_indicies = all_filenames[NUM_TRANSLATIONS*(num_test+num_val):]\n",
    "\n",
    "# construct training and testing data frames \n",
    "test1_df = df_all.loc[test_indices]\n",
    "val1_df = df_all.loc[val_indicies]\n",
    "train1_df = df_all.loc[train_indicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation (filename, path, outfile):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels = 3)\n",
    "    \n",
    "    img_brightness = tf.image.random_brightness(img, max_delta = 0.2)\n",
    "    output_image = tf.image.encode_png(img_brightness)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"brightness1.jpg\"), output_image)\n",
    "    landmarks[filename[0:-4]+\"brightness1.jpg\"] = landmarks[filename]\n",
    "    \n",
    "    img_brightness = tf.image.random_brightness(img, max_delta = 0.2)\n",
    "    output_image = tf.image.encode_png(img_brightness)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"brightness2.jpg\"), output_image)\n",
    "    landmarks[filename[0:-4]+\"brightness2.jpg\"] = landmarks[filename]\n",
    "\n",
    "    img_saturation = tf.image.random_saturation(img, lower = 0.95, upper = 1.05)\n",
    "    output_image = tf.image.encode_png(img_saturation)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"saturation1.jpg\"), output_image)\n",
    "    landmarks[filename[0:-4]+\"saturation1.jpg\"] = landmarks[filename]\n",
    "    \n",
    "    img_saturation = tf.image.random_saturation(img, lower = 0.95, upper = 1.05)\n",
    "    output_image = tf.image.encode_png(img_saturation)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"saturation2.jpg\"), output_image)\n",
    "    landmarks[filename[0:-4]+\"saturation2.jpg\"] = landmarks[filename]\n",
    "    \n",
    "    img_hue = tf.image.random_hue(img, max_delta = 0.2)\n",
    "    output_image = tf.image.encode_png(img_hue)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"hue1.jpg\"), output_image)\n",
    "    landmarks[filename[0:-4]+\"hue1.jpg\"] = landmarks[filename]\n",
    "    \n",
    "    img_hue = tf.image.random_hue(img, max_delta = 0.2)\n",
    "    output_image = tf.image.encode_png(img_hue)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"hue2.jpg\"), output_image)\n",
    "    landmarks[filename[0:-4]+\"hue2.jpg\"] = landmarks[filename]\n",
    "    \n",
    "    img_gray = tf.image.rgb_to_grayscale(img)\n",
    "    output_image = tf.image.encode_png(img_gray)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"gray.jpg\"), output_image)\n",
    "    landmarks[filename[0:-4]+\"gray.jpg\"] = landmarks[filename]\n",
    "    \n",
    "    img_contrast = tf.image.adjust_contrast(img,0.6)\n",
    "    output_image = tf.image.encode_png(img_contrast)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"contrast.jpg\"), output_image)\n",
    "    landmarks[filename[0:-4]+\"contrast.jpg\"] = landmarks[filename]\n",
    "                     \n",
    "    img = tf.image.encode_png(img)\n",
    "    tf.io.write_file(tf.constant(outfile+filename), img) #copy image to same folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# augmentation of images \n",
    "## (except test and val images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testbilder m√ºssen nicht augmentiert werden\n",
    "for i in tqdm(range(0,len(test1_df)), desc = \"Copy test images\", unit = \"images\"):\n",
    "    filename = test1_df['filename'][i]\n",
    "    path = test1_df['image_path'][i]\n",
    "    img = tf.io.read_file(path)\n",
    "    tf.io.write_file(tf.constant(TEST_FOLDER+filename), img)\n",
    "    \n",
    "for i in tqdm(range(0,len(val1_df)), desc = \"Copy validation images\", unit = \"images\"):\n",
    "    filename = val1_df[\"filename\"][i]\n",
    "    path = val1_df['image_path'][i]\n",
    "    img = tf.io.read_file(path)\n",
    "    tf.io.write_file(tf.constant(VAL_FOLDER+filename), img)\n",
    "    #augmentation(filename, val1_df['image_path'][i], VAL_FOLDER)\n",
    "    \n",
    "for i in tqdm(range(0,len(train1_df)), desc = \"Augment train images\", unit = \"images\"):\n",
    "    filename = train1_df['filename'][i]\n",
    "    augmentation(filename, train1_df['image_path'][i], TRAIN_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_landmarks_augmented = pd.DataFrame.from_dict(landmarks).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write new images into text file and read into panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir(TEST_FOLDER)\n",
    "val_files = os.listdir(VAL_FOLDER)\n",
    "train_files = os.listdir(TRAIN_FOLDER)\n",
    "\n",
    "f= open(\"test.txt\",\"w+\")\n",
    "for i in range(len(test_files)):\n",
    "    f.write(f\"{test_files[i]};{test_files[i][3:6]};{TEST_FOLDER}{test_files[i]}\\n\")\n",
    "f.close()\n",
    "\n",
    "f= open(\"val.txt\",\"w+\")\n",
    "for i in range(len(val_files)):\n",
    "    f.write(f\"{val_files[i]};{val_files[i][3:6]};{VAL_FOLDER}{val_files[i]}\\n\")\n",
    "f.close()\n",
    "\n",
    "f= open(\"train.txt\",\"w+\")\n",
    "for i in range(len(train_files)):\n",
    "    f.write(f\"{train_files[i]};{train_files[i][3:6]};{TRAIN_FOLDER}{train_files[i]}\\n\")\n",
    "f.close()\n",
    "\n",
    "# Create a pandas dataframe from a tab separated file \n",
    "test2_df = pd.read_csv(\"test.txt\", sep = ';', names = ['filename','classname','image_path'], encoding = 'unicode_escape')\n",
    "test2_df = test2_df.set_index('filename')\n",
    "\n",
    "val2_df = pd.read_csv(\"val.txt\", sep = ';', names = ['filename','classname','image_path'], encoding = 'unicode_escape')\n",
    "val2_df = val2_df.set_index('filename')\n",
    "\n",
    "train2_df = pd.read_csv(\"train.txt\", sep = ';', names = ['filename','classname', 'image_path'], encoding = 'unicode_escape')\n",
    "train2_df = train2_df.set_index('filename')\n",
    "\n",
    "num_test = test2_df.shape[0]\n",
    "num_train = train2_df.shape[0]\n",
    "num_val = val2_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update test, val and train dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.merge(test1_df, test2_df, how = 'right', on = ['filename', 'classname'])\n",
    "del df_test['image_path_x']\n",
    "df_test.rename(columns={'image_path_y': 'image_path'}, inplace=True)\n",
    "\n",
    "df_val = pd.merge(df_landmarks_augmented, val2_df, how = 'right', left_index=True, right_index=True)\n",
    "# df_val = df_val.set_index('filename')\n",
    "df_val['filename'] = df_val.index\n",
    "\n",
    "df_train = pd.merge(df_landmarks_augmented, train2_df, how = 'right', left_index=True, right_index=True)\n",
    "# df_train = df_train.set_index('filename')\n",
    "df_train['filename'] = df_train.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add 'class' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate class names with a class (0 .. K-1)\n",
    "classnames = df_train['classname'].unique() # all 6 species names\n",
    "K = classnames.size  # 6\n",
    "name2class = dict(zip(classnames, range(K))) # dictionary that maps a name to its index in classnames array\n",
    "print(\"names and classes:\", name2class)\n",
    "\n",
    "# Add a column 'class' to data frame  with the number representing the species name\n",
    "df_test['class'] = df_test['classname'].map(name2class) # new column class with number representing plant name\n",
    "# print a few random example lines\n",
    "#print(df_test.sample(n=5))\n",
    "# Add a column 'class' to data frame  with the number representing the species name\n",
    "df_val['class'] = df_val['classname'].map(name2class) # new column class with number representing plant name\n",
    "# print a few random example lines\n",
    "#print(df_val.sample(n=5))\n",
    "# Add a column 'class' to data frame  with the number representing the species name\n",
    "df_train['class'] = df_train['classname'].map(name2class) # new column class with number representing plant name\n",
    "# print a few random example lines\n",
    "#print(df_train.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save dataframes for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_pickle(TEST_PKL)\n",
    "df_val.to_pickle(VAL_PKL)\n",
    "df_train.to_pickle(TRAIN_PKL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
