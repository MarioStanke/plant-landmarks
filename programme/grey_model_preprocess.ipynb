{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "import pandas as pd\n",
    "import pandas.util\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import math\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = 'images_classification/'\n",
    "\n",
    "TEST_FOLDER = 'classification_grey/test/'\n",
    "TRAIN_FOLDER = 'classification_grey/train/'\n",
    "VAL_FOLDER = 'classification_grey/val/'\n",
    "\n",
    "NUM_TRANSLATIONS = 9 # Number of random tranlations that are applied to each segment\n",
    "NUM_SEGMENTS = 3     # An odd-numbered number of segments that determines which parts are being cut.\n",
    "                     # The segments overlie one another to 50%. \n",
    "        \n",
    "VAL_TRAIN_RATIO = 1/10 #ratio of images in the validation dataset to images in the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load coordinate information of landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob.glob(IMAGE_FOLDER + '*.jpg')\n",
    "\n",
    "# get all unique filenames\n",
    "unique_files = list({key[:-len(\"_s0_t0.jpg\")] for key in image_paths})\n",
    "random.shuffle(unique_files) #randomly shuffle these filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write picture information in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = os.listdir(IMAGE_FOLDER)\n",
    "\n",
    "f= open(\"images.txt\",\"w+\")\n",
    "for i in range(len(filelist)):\n",
    "    f.write(f\"{filelist[i]};{filelist[i][3:6]};{IMAGE_FOLDER}{filelist[i]}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create panda dataframe from textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    classname  \\\n",
      "filename                                        \n",
      "DIPiss13_d_1_s1_t0.jpg                    iss   \n",
      "DIPtriAufschlogersoge_v_4_s0_t7.jpg       tri   \n",
      "DIPtriAufschlogersoge_v_4_s1_t0.jpg       tri   \n",
      "DIPzei3_d_4_s1_t3.jpg                     zei   \n",
      "DIPcom2_v_2_s0_t7.jpg                     com   \n",
      "\n",
      "                                                                            image_path  \n",
      "filename                                                                                \n",
      "DIPiss13_d_1_s1_t0.jpg                    images_classification/DIPiss13_d_1_s1_t0.jpg  \n",
      "DIPtriAufschlogersoge_v_4_s0_t7.jpg  images_classification/DIPtriAufschlogersoge_v_...  \n",
      "DIPtriAufschlogersoge_v_4_s1_t0.jpg  images_classification/DIPtriAufschlogersoge_v_...  \n",
      "DIPzei3_d_4_s1_t3.jpg                      images_classification/DIPzei3_d_4_s1_t3.jpg  \n",
      "DIPcom2_v_2_s0_t7.jpg                      images_classification/DIPcom2_v_2_s0_t7.jpg  \n",
      "shape of data frame:  (19980, 2)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"images.txt\", sep = ';', names = ['filename','classname', 'image_path'], encoding = 'unicode_escape')\n",
    "df = df.set_index('filename')\n",
    "\n",
    "print(df.head())\n",
    "print(\"shape of data frame: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080\n"
     ]
    }
   ],
   "source": [
    "images = np.array([key[len(IMAGE_FOLDER):-len(\"_0_s0_t0.jpg\")] for key in image_paths]) #get all filenames without additional information\n",
    "genet, num_genet = np.unique(images, return_counts=True) #get all clones of one genet\n",
    "num_genet = num_genet // (NUM_SEGMENTS * (NUM_TRANSLATIONS+1)) #update number by taking the segments and translations applied to each image into account\n",
    "lcm_count = np.lcm.reduce(num_genet) #reduce to lowest common multiple\n",
    "test_idx = np.random.randint(low=1, high=lcm_count, size=genet.shape) #randomly determine indices for the test dataset\n",
    "test_idx = (test_idx % num_genet) + 1\n",
    "\n",
    "#get test images\n",
    "test_images = [f\"{genet[i]}_{test_idx[i]}_s{s}_t{t}.jpg\" for i in range(test_idx.shape[0]) for s in range(NUM_SEGMENTS) for t in range(NUM_TRANSLATIONS+1)]\n",
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine train and validation images and create training, validation and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data further into training, validation data frames\n",
    "\n",
    "all_filenames = [image_path[len(IMAGE_FOLDER):] for image_path in image_paths] #get all filenames\n",
    "non_test_filenames = [image for image in all_filenames if image not in test_images] #get all filenames apart from the ones that belong to test images\n",
    "random.shuffle(non_test_filenames) #shuffle these filenames randomly\n",
    "\n",
    "num_val = int(VAL_TRAIN_RATIO * len(non_test_filenames)) #set the number of images in the validation data set\n",
    "\n",
    "val_images = non_test_filenames[:num_val] #get validation images\n",
    "train_images = non_test_filenames[num_val:] #get training images\n",
    "\n",
    "# construct training, validation and testing data frames \n",
    "test_df = df.loc[test_images]\n",
    "val_df = df.loc[val_images]\n",
    "train_df = df.loc[train_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation (filename, path, outfile):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels = 3)\n",
    "    \n",
    "    img_brightness = tf.image.random_brightness(img, max_delta = 0.2)\n",
    "    output_image = tf.image.encode_png(img_brightness)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"brightness1.jpg\"), output_image)\n",
    "    \n",
    "    img_brightness = tf.image.random_brightness(img, max_delta = 0.2)\n",
    "    output_image = tf.image.encode_png(img_brightness)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"brightness2.jpg\"), output_image)\n",
    "    \n",
    "    img_contrast = tf.image.adjust_contrast(img,0.6)\n",
    "    output_image = tf.image.encode_png(img_contrast)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-4]+\"contrast.jpg\"), output_image)\n",
    "                     \n",
    "    img = tf.image.encode_png(img)\n",
    "    tf.io.write_file(tf.constant(outfile+filename), img) #copy image to same folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# augmentation of images \n",
    "## (only training images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform test images to greyscale\n",
    "for i in range(0,len(test_images)):\n",
    "    filename = test_images[i]\n",
    "    path = IMAGE_FOLDER+filename\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels = 3)\n",
    "    img_gray = tf.image.rgb_to_grayscale(img)\n",
    "    output_image = tf.image.encode_png(img_gray)\n",
    "    tf.io.write_file(tf.constant(TEST_FOLDER+filename), output_image)\n",
    "\n",
    "#transform validation images to greyscale\n",
    "for i in range(0,len(val_images)):\n",
    "    filename = val_images[i]\n",
    "    path = IMAGE_FOLDER+filename\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels = 3)\n",
    "    img_gray = tf.image.rgb_to_grayscale(img)\n",
    "    output_image = tf.image.encode_png(img_gray)\n",
    "    tf.io.write_file(tf.constant(VAL_FOLDER+filename), output_image)\n",
    "    \n",
    "#transform training images to greyscale\n",
    "for i in range(0,len(train_images)):\n",
    "    filename = train_images[i]\n",
    "    path = IMAGE_FOLDER+filename\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels = 3)\n",
    "    img_gray = tf.image.rgb_to_grayscale(img)\n",
    "    output_image = tf.image.encode_png(img_gray)\n",
    "    tf.io.write_file(tf.constant(TRAIN_FOLDER+filename), output_image)\n",
    "\n",
    "#augment training images\n",
    "train_files = os.listdir(TRAIN_FOLDER)\n",
    "for i in range(len(train_files)):\n",
    "    filename = train_files[i]\n",
    "    augmentation(filename, TRAIN_FOLDER+filename, TRAIN_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write new images into text file and read into panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir(TEST_FOLDER)\n",
    "val_files = os.listdir(VAL_FOLDER)\n",
    "train_files = os.listdir(TRAIN_FOLDER)\n",
    "\n",
    "random.shuffle(train_files)\n",
    "\n",
    "f= open(\"test.txt\",\"w+\")\n",
    "for i in range(len(test_files)):\n",
    "    f.write(f\"{test_files[i]};{test_files[i][3:6]};{TEST_FOLDER}{test_files[i]}\\n\")\n",
    "f.close()\n",
    "\n",
    "f= open(\"val.txt\",\"w+\")\n",
    "for i in range(len(val_files)):\n",
    "    f.write(f\"{val_files[i]};{val_files[i][3:6]};{VAL_FOLDER}{val_files[i]}\\n\")\n",
    "f.close()\n",
    "\n",
    "f= open(\"train.txt\",\"w+\")\n",
    "for i in range(len(train_files)):\n",
    "    f.write(f\"{train_files[i]};{train_files[i][3:6]};{TRAIN_FOLDER}{train_files[i]}\\n\")\n",
    "f.close()\n",
    "\n",
    "# Create a pandas dataframe from a tab separated file \n",
    "test_df = pd.read_csv(\"test.txt\", sep = ';', names = ['filename','classname','image_path'], encoding = 'unicode_escape')\n",
    "test_df = test_df.set_index('filename')\n",
    "\n",
    "val_df = pd.read_csv(\"val.txt\", sep = ';', names = ['filename','classname','image_path'], encoding = 'unicode_escape')\n",
    "val_df = val_df.set_index('filename')\n",
    "\n",
    "train_df = pd.read_csv(\"train.txt\", sep = ';', names = ['filename','classname', 'image_path'], encoding = 'unicode_escape')\n",
    "train_df = train_df.set_index('filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add 'class' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names and classes: {'alp': 0, 'iss': 1, 'zei': 2, 'oel': 3, 'com': 4, 'tri': 5}\n"
     ]
    }
   ],
   "source": [
    "# associate class names with a class (0 .. K-1)\n",
    "classnames = train_df['classname'].unique() # all 6 species names\n",
    "K = classnames.size  # 6\n",
    "name2class = dict(zip(classnames, range(K))) # dictionary that maps a name to its index in classnames array\n",
    "print(\"names and classes:\", name2class)\n",
    "\n",
    "# Add a column 'class' to data frame  with the number representing the species name\n",
    "test_df['class'] = test_df['classname'].map(name2class) # new column class with number representing plant name\n",
    "\n",
    "# Add a column 'class' to data frame  with the number representing the species name\n",
    "val_df['class'] = val_df['classname'].map(name2class) # new column class with number representing plant name\n",
    "# Add a column 'class' to data frame  with the number representing the species name\n",
    "train_df['class'] = train_df['classname'].map(name2class) # new column class with number representing plant name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save dataframes for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pickle('grey_classification_test.pkl')\n",
    "val_df.to_pickle('grey_classification_val.pkl')\n",
    "train_df.to_pickle('grey_classification_train.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
