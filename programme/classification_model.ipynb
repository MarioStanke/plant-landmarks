{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Reshape\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import pandas as pd\n",
    "import pandas.util\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (105,75)    # width and height of all images (resize, if required)\n",
    "BATCH_SIZE = 32  # for training and prediction\n",
    "EPOCHS = 30 #number of epochs for training\n",
    "\n",
    "HP_DROPOUT_CONV = hp.HParam('dropout_conv', hp.Discrete([.25,.3,.35,.4])) \n",
    "HP_DROPOUT_DENSE = hp.HParam('dropout_dense', hp.Discrete([.25,.3,.35,.4]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "METRIC_ACCURACY = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get test, train, validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle(\"classification_test.pkl\")\n",
    "train_df = pd.read_pickle(\"classification_train.pkl\")\n",
    "val_df = pd.read_pickle(\"classification_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map a filename to an one-hot encode label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_array(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_png(img, channels = 3)\n",
    "    # now img is 3 dim array of numbers in {0,..., 255}\n",
    "    img = tf.cast(img, dtype = tf.float32) / 255. # scale to floating point number in [0,1] EVTL IN [-1,1] skalieren??????\n",
    " \n",
    "    # one-hot encode the label, e.g. 3 becomes [0,0,0,1,0,0,0,0,0,0,0,0]\n",
    "    label = tf.one_hot(label, depth = 6) #6 classes\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a tf dataset of images from a pd data frame of file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df):\n",
    "    # first, make dataset with just the relevant: path and class\n",
    "    ds_path = tf.data.Dataset.from_tensor_slices((df['image_path'], df['class']))\n",
    "\n",
    "    # convert to data set with actual images\n",
    "    ds = ds_path.map(path_to_array)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    return ds\n",
    "\n",
    "test_ds  = make_dataset(test_df)\n",
    "val_ds   = make_dataset(val_df)\n",
    "train_ds = make_dataset(train_df)\n",
    "train_ds = train_ds.repeat().prefetch(tf.data.experimental.AUTOTUNE) # infinitely repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hparams):\n",
    "    \n",
    "    kernel_size = (3, 3)\n",
    "    pool_size   = (2, 2)\n",
    "    first_filters  = 32\n",
    "    second_filters = 64\n",
    "    third_filters  = 128\n",
    "    dropout_conv  = hparams[HP_DROPOUT_CONV]\n",
    "    dropout_dense = hparams[HP_DROPOUT_DENSE]\n",
    "\n",
    "    model = tf.keras.models.Sequential() # sequential stack of layers\n",
    "\n",
    "    model.add( BatchNormalization(input_shape = (IMG_SIZE[1],IMG_SIZE[0], 3)))\n",
    "    model.add( Conv2D (first_filters, kernel_size, activation = 'relu')) # convolutional layer + activation layer\n",
    "    model.add( Conv2D (first_filters, kernel_size, activation = 'relu')) \n",
    "    model.add( Conv2D (first_filters, kernel_size, activation = 'relu')) \n",
    "    model.add( MaxPooling2D (pool_size = pool_size)) #Pooling layer\n",
    "    model.add( Dropout (dropout_conv)) #Dropout layer\n",
    "\n",
    "    model.add( Conv2D (second_filters, kernel_size, activation ='relu')) \n",
    "    model.add( Conv2D (second_filters, kernel_size, activation ='relu')) \n",
    "    model.add( Conv2D (second_filters, kernel_size, activation ='relu'))\n",
    "    model.add( MaxPooling2D (pool_size = pool_size))\n",
    "    model.add( Dropout (dropout_conv))\n",
    "\n",
    "    model.add( Conv2D (third_filters, kernel_size, activation ='relu'))\n",
    "    model.add( Conv2D (third_filters, kernel_size, activation ='relu'))\n",
    "    model.add( Conv2D (third_filters, kernel_size, activation ='relu'))\n",
    "    model.add( MaxPooling2D (pool_size = pool_size))\n",
    "    model.add( Dropout (dropout_conv))\n",
    "\n",
    "    model.add( Flatten())\n",
    "    model.add( Dense (256, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.001))) #dense layer + activation layer\n",
    "    model.add( Dropout (dropout_dense)) #Dropout layer\n",
    "    model.add( Dense(6, activation = 'softmax') ) # activation layer\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model via hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_df)\n",
    "session_num = 0\n",
    "\n",
    "\n",
    "for dr_conv in HP_DROPOUT_CONV.domain.values:\n",
    "    for dr_dense in HP_DROPOUT_DENSE.domain.values:\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "\n",
    "            hparams = {\n",
    "                HP_DROPOUT_CONV: dr_conv,\n",
    "                HP_DROPOUT_DENSE: dr_dense,\n",
    "                HP_OPTIMIZER: optimizer,\n",
    "            }\n",
    "            run_name = f\"run-{session_num}\"\n",
    "            run_dir = 'logs/hparam_tuning/' + run_name\n",
    "            print(f'--- Starting trial: {run_name}')\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "\n",
    "            with tf.summary.create_file_writer(run_dir).as_default():\n",
    "                hp.hparams(hparams)  # record the values used in this trial\n",
    "                model = create_model(hparams)\n",
    "                # Function to decrease learning rate by 'factor'\n",
    "                # when there has been no significant improvement in the last 'patience' epochs.\n",
    "                reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', factor = 0.75, patience = 4, verbose = 1)\n",
    "                         \n",
    "                # define the loss, optimization algorithm and prepare the model for gradient computation \n",
    "                model.compile(optimizer = hparams[HP_OPTIMIZER],\n",
    "                              loss = 'categorical_crossentropy', metrics = [METRIC_ACCURACY]) \n",
    "\n",
    "                # Callbacks: What should be done during (long) training?\n",
    "                modelfname = f\"model_checkpoints/classification_{dr_conv}_{dr_dense}_{optimizer}.h5\"\n",
    "                # Function to store model to file, if validation loss has a new record\n",
    "                # Check always after having seen at least another save_freq examples.\n",
    "                checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                    modelfname, monitor = 'val_loss', mode = 'min', \n",
    "                    save_best_only = True, verbose = 1)\n",
    "\n",
    "                history = model.fit_generator(\n",
    "                    train_ds, epochs = EPOCHS, \n",
    "                    steps_per_epoch = num_train / BATCH_SIZE, #would use each example once on average\n",
    "                    validation_data = val_ds, verbose = 1,\n",
    "                    callbacks = [checkpoint,reduce_lr]\n",
    "                )\n",
    "\n",
    "                # Das \"beste\" Model (kleinster val_loss) fuer die eval. benutzen\n",
    "                model.load_weights(modelfname)\n",
    "                _, accuracy = model.evaluate(test_ds)        \n",
    "                tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "                session_num += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
