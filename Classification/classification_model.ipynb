{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following ten imports have to be installed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import os \n",
    "import shutil\n",
    "import pandas.util\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (105,75)    # width and height of all images (resize, if required)\n",
    "BATCH_SIZE = 32  # for training and prediction\n",
    "EPOCHS = 20 #number of epochs for training\n",
    "NUM_RUNS = 1 #how many times should the training be repeated with the same hyperparameters?\n",
    "\n",
    "HP_DROPOUT_CONV = hp.HParam('dropout_conv', hp.Discrete([.3]))\n",
    "HP_DROPOUT_DENSE = hp.HParam('dropout_dense', hp.Discrete([0.3]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "LOG_DIR = 'logs/'\n",
    "\n",
    "TEST_PKL = \"files/classification_test.pkl\"\n",
    "TRAIN_PKL = \"files/classification_train.pkl\"\n",
    "VAL_PKL = \"files/classification_val.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if all files and folder exist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the LOG_DIR path does not exist, create it. \n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "# However, if the path exists and has been used before, the content has to be deleted and the directory created again!\n",
    "if os.path.exists(LOG_DIR):\n",
    "    shutil.rmtree(LOG_DIR)\n",
    "    os.mkdir(LOG_DIR)\n",
    "\n",
    "#check if the TEST_PKL, TRAIN_PKL, VAL_PKL files exist. If not: raise an error.\n",
    "if not os.path.exists(TEST_PKL):\n",
    "    raise FileNotFoundError('The model preprocess (classification_model_preprocess.ipynb) has to be conducted first or the name of the TEST_PKL has to be adjusted.')\n",
    "if not os.path.exists(TRAIN_PKL):\n",
    "    raise FileNotFoundError('The model preprocess (classification_model_preprocess.ipynb) has to be conducted first or the name of the IMAGE_FOLDER has to be adjusted.')\n",
    "if not os.path.exists(VAL_PKL):\n",
    "    raise FileNotFoundError('The model preprocess (classification_model_preprocess.ipynb) has to be conducted first or the name of the IMAGE_FOLDER has to be adjusted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get test, train, validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle(TEST_PKL)\n",
    "train_df = pd.read_pickle(TRAIN_PKL)\n",
    "val_df = pd.read_pickle(VAL_PKL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map a filename to an image tensor and one-hot encode label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_array(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_png(img, channels = 3)\n",
    "    # now img is 3 dim array of numbers in {0,..., 255}\n",
    "    img = tf.cast(img, dtype = tf.float32) / 255. # scale to floating point number in [0,1] \n",
    " \n",
    "    # one-hot encode the label, e.g. 3 becomes [0,0,0,1,0,0,0,0,0,0,0,0]\n",
    "    label = tf.one_hot(label, depth = 6) #6 classes\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a tf dataset of images from a pd data frame of file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df):\n",
    "    # first, make dataset with just the relevant: path and class\n",
    "    ds_path = tf.data.Dataset.from_tensor_slices((df['image_path'], df['class']))\n",
    "\n",
    "    # convert to data set with actual images\n",
    "    ds = ds_path.map(path_to_array)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    return ds\n",
    "\n",
    "test_ds  = make_dataset(test_df)\n",
    "val_ds   = make_dataset(val_df)\n",
    "train_ds = make_dataset(train_df)\n",
    "train_ds = train_ds.repeat().prefetch(tf.data.experimental.AUTOTUNE) # infinitely repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hparams):\n",
    "    \n",
    "    kernel_size = (3, 3)\n",
    "    pool_size   = (2, 2)\n",
    "    first_filters  = 32\n",
    "    second_filters = 64\n",
    "    third_filters  = 128\n",
    "    dropout_conv  = hparams[HP_DROPOUT_CONV]\n",
    "    dropout_dense = hparams[HP_DROPOUT_DENSE]\n",
    "\n",
    "    model = tf.keras.models.Sequential() # sequential stack of layers\n",
    "\n",
    "    model.add( BatchNormalization(input_shape = (IMG_SIZE[1],IMG_SIZE[0], 3)))\n",
    "    model.add( Conv2D (first_filters, kernel_size, activation = 'relu')) # convolutional layer + activation layer\n",
    "    model.add( Conv2D (first_filters, kernel_size, activation = 'relu')) \n",
    "    model.add( Conv2D (first_filters, kernel_size, activation = 'relu')) \n",
    "    model.add( MaxPooling2D (pool_size = pool_size)) #Pooling layer\n",
    "    model.add( Dropout (dropout_conv)) #Dropout layer\n",
    "\n",
    "    model.add( Conv2D (second_filters, kernel_size, activation ='relu')) \n",
    "    model.add( Conv2D (second_filters, kernel_size, activation ='relu')) \n",
    "    model.add( Conv2D (second_filters, kernel_size, activation ='relu'))\n",
    "    model.add( MaxPooling2D (pool_size = pool_size))\n",
    "    model.add( Dropout (dropout_conv))\n",
    "\n",
    "    model.add( Conv2D (third_filters, kernel_size, activation ='relu'))\n",
    "    model.add( Conv2D (third_filters, kernel_size, activation ='relu'))\n",
    "    model.add( Conv2D (third_filters, kernel_size, activation ='relu'))\n",
    "    model.add( MaxPooling2D (pool_size = pool_size))\n",
    "    model.add( Dropout (dropout_conv))\n",
    "\n",
    "    model.add( Flatten())\n",
    "    model.add( Dense (256, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.001))) #dense layer + activation layer\n",
    "    model.add( Dropout (dropout_dense)) #Dropout layer\n",
    "    model.add( Dense(6, activation = 'softmax') ) # activation layer\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_df)\n",
    "session_num = 0\n",
    "\n",
    "for session_num in range(NUM_RUNS):\n",
    "    for dr_conv in HP_DROPOUT_CONV.domain.values:\n",
    "        for dr_dense in HP_DROPOUT_DENSE.domain.values:\n",
    "            for optimizer in HP_OPTIMIZER.domain.values:\n",
    "\n",
    "                hparams = {\n",
    "                    HP_DROPOUT_CONV: dr_conv,\n",
    "                    HP_DROPOUT_DENSE: dr_dense,\n",
    "                    HP_OPTIMIZER: optimizer,\n",
    "                }\n",
    "                run_name = f\"run-{session_num}\"\n",
    "                run_dir = f\"{LOG_DIR}{run_name}\"\n",
    "                print(f'--- Starting trial: {run_name}')\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "\n",
    "                with tf.summary.create_file_writer(run_dir).as_default():\n",
    "                    hp.hparams(hparams)  # record the values used in this trial\n",
    "                    model = create_model(hparams)\n",
    "                    # Function to decrease learning rate by 'factor'\n",
    "                    # when there has been no significant improvement in the last 'patience' epochs.\n",
    "                    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', factor = 0.75, patience = 4, verbose = 1)\n",
    "                    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=1, update_freq='batch')\n",
    "\n",
    "                    # define the loss, optimization algorithm and prepare the model for gradient computation \n",
    "                    model.compile(optimizer = hparams[HP_OPTIMIZER],\n",
    "                                  loss = 'categorical_crossentropy', metrics = [METRIC_ACCURACY]) \n",
    "\n",
    "                    # Callbacks: What should be done during training?\n",
    "                    modelfname = f\"model_checkpoints/classification_r{session_num}.h5\"\n",
    "                    # Function to store model to file, if validation loss has a new record\n",
    "                    # Check always after having seen at least another save_freq examples.\n",
    "                    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        modelfname, monitor = 'val_loss', mode = 'min', \n",
    "                        save_best_only = True, verbose = 1)\n",
    "                    \n",
    "                    #evaluate the model after each epoch\n",
    "                    test_on_epoch_end = tf.keras.callbacks.LambdaCallback(\n",
    "                        on_epoch_end=lambda epoch,logs: model.evaluate(test_ds, verbose = 1)\n",
    "                    )\n",
    "                    \n",
    "                    history = model.fit_generator(\n",
    "                        train_ds, epochs = EPOCHS, \n",
    "                        steps_per_epoch = num_train / BATCH_SIZE, #would use each example once on average\n",
    "                        validation_data = val_ds, verbose = 1,\n",
    "                        callbacks = [checkpoint,tensorboard_callback,test_on_epoch_end,]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set and get parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {HP_DROPOUT_CONV:0.3, HP_DROPOUT_DENSE:0.3}\n",
    "hp.hparams(hparams)\n",
    "model = create_model(hparams)\n",
    "model.load_weights(\"model_checkpoints/classification_best.h5\") #this is the h5 file of my best run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_ds)\n",
    "yhat = prediction.argmax(axis = 1)\n",
    "if 'pred' not in test_df:\n",
    "    test_df.insert(3, 'pred',  prediction.argmax(axis = 1))\n",
    "if 'confidence' not in test_df:\n",
    "    test_df.insert(4, 'confidence',  prediction.max(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a confusion matrix with numbers as entries\n",
    "con_mat = tf.math.confusion_matrix(labels=test_df['class'], predictions=test_df['pred']).numpy()\n",
    "classnames = train_df['classname'].unique() \n",
    "K = classnames.size  # 6\n",
    "name2class = dict(zip(classnames, range(K))) \n",
    "\n",
    "#create a dataframe as con_mat displays only a numpy array, meaning without column and row description\n",
    "con_mat_df = pd.DataFrame(con_mat,\n",
    "                     index = classnames, \n",
    "                     columns = classnames)\n",
    "\n",
    "#normalized confusion matrix: only entries between 0 and 1\n",
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "con_mat_norm_df = pd.DataFrame(con_mat_norm,\n",
    "                     index = classnames, \n",
    "                     columns = classnames)\n",
    "print(con_mat_norm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to print the confusion matrix as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import text\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (13,9), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\".2f\", vmin=0, vmax=1, center = 0.49,cmap=sns.cubehelix_palette(dark=0.4, light=1, as_cmap=True,rot=-.2,start=0))\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    heatmap.set_ylim(0,6)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    heatmap.text(7,4.5,f\"Number of images per label:\\n alp: {1200}\\n tri: {420}\\n zei: {660}\\n iss: {1020}\\n com: {420}\\n oel: {360} \")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = print_confusion_matrix(con_mat_norm_df, classnames) #print the normalized confusion matrix, otherwise change con_mat_norm_df to con_mat_df\n",
    "#fig.savefig(\"heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the classification report and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(test_df['class'], test_df['pred'], target_names = classnames)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine false and correct classified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_classified_df = test_df[(test_df['class'] != test_df['pred'])]\n",
    "correct_classified_df = test_df[(test_df['class'] == test_df['pred'])]\n",
    "numfalse = false_classified_df.shape[0]\n",
    "print(f\"number of false classified = {numfalse}\")\n",
    "numcorrect =correct_classified_df.shape[0]\n",
    "print(f\"number of correct classified = {numcorrect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some false classified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_show = min(numfalse,50) # show at most 50 false examples\n",
    "ncols = 4\n",
    "nrows = math.ceil(num_show / ncols) # round up\n",
    "nrows = min(nrows, 15) # at most 15 rows\n",
    "f, ax = plt.subplots(nrows, ncols, figsize = (3 * ncols, 3 * nrows))\n",
    "for k in range(num_show):\n",
    "    i = math.floor(k / ncols) # row\n",
    "    j = k % ncols # column\n",
    "    record = false_classified_df.iloc[k]\n",
    "    path = record['image_path']\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    ax[i, j].imshow(img)\n",
    "    ax[i, j].set_title(classnames[record['class']] + \" predicted as\\n\"+ \n",
    "                       str(classnames[record['pred']]) + \" with conf. \"\n",
    "                       + str(np.round(record['confidence'], 3)),fontsize=12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
