{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import glob\n",
    "import shutil\n",
    "import pandas.util\n",
    "\n",
    "#the following five imports have to be installed\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = 'images/images_classification/'\n",
    "\n",
    "TEST_FOLDER = 'images/test/'\n",
    "TRAIN_FOLDER = 'images/train/'\n",
    "VAL_FOLDER = 'images/val/'\n",
    "\n",
    "NUM_TRANSLATIONS = 9 # Number of random tranlations that are applied to each segment\n",
    "NUM_SEGMENTS = 3     # An odd-numbered number of segments that determines which parts are being cut.\n",
    "                     # The segments overlie one another to 50%. \n",
    "        \n",
    "VAL_TRAIN_RATIO = 1/10 #ratio of images in the validation dataset to images in the train dataset\n",
    "\n",
    "TEST_PKL = 'files/classification_test.pkl'\n",
    "TRAIN_PKL = 'files/classification_train.pkl'\n",
    "VAL_PKL = 'files/classification_val.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if all files and folder exist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the IMAGE_FOLDER exists, if not: raise an error.\n",
    "if not os.path.exists(IMAGE_FOLDER):\n",
    "    raise FileNotFoundError('The preprocessing of the images (classification_image_preprocessing.ipynb) has to be conducted first or the name of the IMAGE_FOLDER has to be adjusted.')\n",
    "\n",
    "#check if the directories TEST_FOLDER, TRAIN_FOLDER, VAL_FOLDER exist, if not: create them.\n",
    "#Otherwise delete its contents\n",
    "if not os.path.exists(TEST_FOLDER):\n",
    "    os.mkdir(TEST_FOLDER)\n",
    "else:\n",
    "    shutil.rmtree(TEST_FOLDER)\n",
    "    os.mkdir(TEST_FOLDER)\n",
    "    \n",
    "if not os.path.exists(TRAIN_FOLDER):\n",
    "    os.makedirs(TRAIN_FOLDER)\n",
    "else:\n",
    "    shutil.rmtree(TRAIN_FOLDER)\n",
    "    os.mkdir(TRAIN_FOLDER)\n",
    "    \n",
    "if not os.path.exists(VAL_FOLDER):\n",
    "    os.makedirs(VAL_FOLDER)\n",
    "else:\n",
    "    shutil.rmtree(VAL_FOLDER)\n",
    "    os.mkdir(VAL_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write picture information in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = os.listdir(IMAGE_FOLDER)\n",
    "\n",
    "f= open(\"images.txt\",\"w+\")\n",
    "for i in range(len(filelist)):\n",
    "    f.write(f\"{filelist[i]};{filelist[i][3:6]};{IMAGE_FOLDER}{filelist[i]}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create panda dataframe from textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"images.txt\", sep = ';', names = ['filename','classname', 'image_path'], encoding = 'unicode_escape')\n",
    "df = df.set_index('filename')\n",
    "\n",
    "print(df.head())\n",
    "print(\"shape of data frame: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob.glob(IMAGE_FOLDER + '*.jpg')\n",
    "images = np.array([key[len(IMAGE_FOLDER):-len(\"_0_s0_t0.jpg\")] for key in image_paths]) #get all image names without information of number of translation, segment and clone\n",
    "genet, num_genet = np.unique(images, return_counts=True) # get all unique genets but also count their number in 'images'\n",
    "num_genet = num_genet // (NUM_SEGMENTS * (NUM_TRANSLATIONS+1)) #floor division of that number by the number of translations and segments to get the original number\n",
    "lcm_count = np.lcm.reduce(num_genet) #calculate the lowest common multiple of all numbers of genets\n",
    "test_idx = np.random.randint(low=1, high=lcm_count, size=genet.shape) #define the interval of the number of images that are randomly chosen for the dataset\n",
    "test_idx = (test_idx % num_genet) + 1 #modulo calculation to respect the number of genets in the choice\n",
    "\n",
    "#get the test images by iterating over the index and the number of segments and translations\n",
    "test_images = [f\"{genet[i]}_{test_idx[i]}_s{s}_t{t}.jpg\" for i in range(test_idx.shape[0]) for s in range(NUM_SEGMENTS) for t in range(NUM_TRANSLATIONS+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine train and validation images and create training, validation and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data further into training, validation data frames\n",
    "all_filenames = [image_path[len(IMAGE_FOLDER):] for image_path in image_paths] #get all filenames\n",
    "non_test_filenames = [image for image in all_filenames if image not in test_images] #get all filenames apart from the ones that belong to test images\n",
    "random.shuffle(non_test_filenames) #shuffle these filenames randomly\n",
    "\n",
    "num_val = int(VAL_TRAIN_RATIO * len(non_test_filenames)) #set the number of images in the validation data set\n",
    "\n",
    "val_images = non_test_filenames[:num_val] #get validation images\n",
    "train_images = non_test_filenames[num_val:] #get training images\n",
    "\n",
    "# construct training, validation and testing data frames \n",
    "test_df = df.loc[test_images]\n",
    "val_df = df.loc[val_images]\n",
    "train_df = df.loc[train_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation (filename, path, outfile):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels = 3)\n",
    "    \n",
    "    len_end = len(\".jpg\")\n",
    "    \n",
    "    img_brightness = tf.image.random_brightness(img, max_delta = 0.2)\n",
    "    output_image = tf.image.encode_png(img_brightness)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[:-len_end]+\"brightness1.jpg\"), output_image)\n",
    "    \n",
    "    img_brightness = tf.image.random_brightness(img, max_delta = 0.2)\n",
    "    output_image = tf.image.encode_png(img_brightness)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-len_end]+\"brightness2.jpg\"), output_image)\n",
    "\n",
    "    img_saturation = tf.image.random_saturation(img, lower = 0.95, upper = 1.05)\n",
    "    output_image = tf.image.encode_png(img_saturation)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-len_end]+\"saturation1.jpg\"), output_image)\n",
    "    \n",
    "    img_saturation = tf.image.random_saturation(img, lower = 0.95, upper = 1.05)\n",
    "    output_image = tf.image.encode_png(img_saturation)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-len_end]+\"saturation2.jpg\"), output_image)\n",
    "    \n",
    "    img_hue = tf.image.random_hue(img, max_delta = 0.2)\n",
    "    output_image = tf.image.encode_png(img_hue)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-len_end]+\"hue1.jpg\"), output_image)\n",
    "    \n",
    "    img_hue = tf.image.random_hue(img, max_delta = 0.2)\n",
    "    output_image = tf.image.encode_png(img_hue)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-len_end]+\"hue2.jpg\"), output_image)\n",
    "    \n",
    "    img_gray = tf.image.rgb_to_grayscale(img)\n",
    "    output_image = tf.image.encode_png(img_gray)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-len_end]+\"gray.jpg\"), output_image)\n",
    "    \n",
    "    img_contrast = tf.image.adjust_contrast(img,0.6)\n",
    "    output_image = tf.image.encode_png(img_contrast)\n",
    "    tf.io.write_file(tf.constant(outfile+filename[0:-len_end]+\"contrast.jpg\"), output_image)\n",
    "                     \n",
    "    img = tf.image.encode_png(img)\n",
    "    tf.io.write_file(tf.constant(outfile+filename), img) #copy image to same folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation of images \n",
    "## (only training images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0,len(test_images)), desc = \"Copy test images\", unit = \"images\"):\n",
    "    filename = test_images[i]\n",
    "    path = IMAGE_FOLDER + filename\n",
    "    img = tf.io.read_file(path)\n",
    "    tf.io.write_file(tf.constant(TEST_FOLDER+filename), img)\n",
    "    \n",
    "for i in tqdm(range(0,len(val_images)), desc = \"Copy val images \", unit = \"images\"):\n",
    "    filename = val_images[i]\n",
    "    path = IMAGE_FOLDER + filename\n",
    "    img = tf.io.read_file(path)\n",
    "    tf.io.write_file(tf.constant(VAL_FOLDER+filename), img)\n",
    "    \n",
    "for i in tqdm(range(0,len(train_images)), desc = \"Augment train images\", unit = \"images\"):\n",
    "    filename = train_images[i]\n",
    "    augmentation(filename, IMAGE_FOLDER+filename, TRAIN_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write new images in to text file and read into panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir(TEST_FOLDER)\n",
    "val_files = os.listdir(VAL_FOLDER)\n",
    "train_files = os.listdir(TRAIN_FOLDER)\n",
    "\n",
    "random.shuffle(train_files)\n",
    "\n",
    "f= open(\"test.txt\",\"w+\")\n",
    "for i in range(len(test_files)):\n",
    "    f.write(f\"{test_files[i]};{test_files[i][3:6]};{TEST_FOLDER}{test_files[i]}\\n\")\n",
    "f.close()\n",
    "\n",
    "f= open(\"val.txt\",\"w+\")\n",
    "for i in range(len(val_files)):\n",
    "    f.write(f\"{val_files[i]};{val_files[i][3:6]};{VAL_FOLDER}{val_files[i]}\\n\")\n",
    "f.close()\n",
    "\n",
    "f= open(\"train.txt\",\"w+\")\n",
    "for i in range(len(train_files)):\n",
    "    f.write(f\"{train_files[i]};{train_files[i][3:6]};{TRAIN_FOLDER}{train_files[i]}\\n\")\n",
    "f.close()\n",
    "\n",
    "# Create a pandas dataframe from a tab separated file \n",
    "test_df = pd.read_csv(\"test.txt\", sep = ';', names = ['filename','classname','image_path'], encoding = 'unicode_escape')\n",
    "test_df = test_df.set_index('filename')\n",
    "\n",
    "val_df = pd.read_csv(\"val.txt\", sep = ';', names = ['filename','classname','image_path'], encoding = 'unicode_escape')\n",
    "val_df = val_df.set_index('filename')\n",
    "\n",
    "train_df = pd.read_csv(\"train.txt\", sep = ';', names = ['filename','classname', 'image_path'], encoding = 'unicode_escape')\n",
    "train_df = train_df.set_index('filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 'class' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate class names with a class (0 .. K-1)\n",
    "classnames = train_df['classname'].unique() # all 6 species names\n",
    "K = classnames.size  # 6\n",
    "name2class = dict(zip(classnames, range(K))) # dictionary that maps a name to its index in classnames array\n",
    "print(\"names and classes:\", name2class)\n",
    "\n",
    "# Add a column 'class' to data frame  with the number representing the species name\n",
    "test_df['class'] = test_df['classname'].map(name2class) # new column class with number representing plant name\n",
    "\n",
    "# Add a column 'class' to data frame  with the number representing the species name\n",
    "val_df['class'] = val_df['classname'].map(name2class) # new column class with number representing plant name\n",
    "# Add a column 'class' to data frame  with the number representing the species name\n",
    "train_df['class'] = train_df['classname'].map(name2class) # new column class with number representing plant name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataframes for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pickle(TEST_PKL)\n",
    "val_df.to_pickle(VAL_PKL)\n",
    "train_df.to_pickle(TRAIN_PKL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
